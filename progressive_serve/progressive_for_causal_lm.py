"""
Universal ProgressiveForCausalLM with Dual-Path Design
vLLM v0.8.0 Compatible (v0 engine) - Supports All Decoder-Only Models

âœ… ëª¨ë“  Decoder-only ëª¨ë¸ ì§€ì›
âœ… Path A/B ë‘˜ ë‹¤ í•­ìƒ ê³„ì‚° (CUDA Graph topology ë¶ˆë³€)
âœ… Alphaë¡œ ê²½ë¡œ ì„ íƒ
âœ… prune_log.json ê¸°ë°˜ ìë™ ë ˆì´ì–´ ê²°ì •
âœ… v0 engine: compute_logits(hidden_states, sampling_metadata) + sample()

<í•µì‹¬ ê¸°ëŠ¥>
_load_prune_log: ëª¨ë¸ í´ë”ì˜ prune_log.jsonì„ ì½ì–´ì„œ "ì´ë²ˆì—” ëª‡ ë²ˆ ë ˆì´ì–´ë¥¼ ë„ê³  ì‹œì‘í• ê¹Œ?"ë¥¼ ê²°ì •

load_weights: model_config.pyì˜ ì •ë³´ë¥¼ ì´ìš©í•´ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì—ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì½ì–´ì™€ ëª¨ë¸ì— ì§‘ì–´ë„£ìŒ.
ë§Œì•½ êº¼ì§„ ë ˆì´ì–´(Inactive)ë¼ë©´ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ì±„ì›Œ ë©”ëª¨ë¦¬ë¥¼ ì•„ë¼ê±°ë‚˜ ì´ˆê¸°í™” ì´ìŠˆë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.

advance_to_stageX: ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°ˆ ë•Œ í•„ìš”í•œ ì¶”ê°€ ê°€ì¤‘ì¹˜ íŒŒì¼(Safetensors)ì„ ë¡œë“œí•˜ê³ , model.activate_layersë¥¼ í˜¸ì¶œ
"""

from typing import Optional, List, Iterable, Tuple, Any, Dict
import torch
import torch.nn as nn
import sys
from vllm.config import VllmConfig
from vllm.model_executor.layers.logits_processor import LogitsProcessor
from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
from vllm.model_executor.layers.vocab_parallel_embedding import (
    ParallelLMHead,
    VocabParallelEmbedding,
)
from vllm.model_executor.sampling_metadata import SamplingMetadata

# Weight loader
try:
    from vllm.model_executor.model_loader.weight_utils import default_weight_loader
except ImportError:
    default_weight_loader = None

# Universal Dual-Path implementation (same directory, no need for sys.path)
from progressive_model_dual_path import ProgressiveModelDualPath
from model_config import get_model_type, get_weight_pattern


class ProgressiveForCausalLM(nn.Module):
    """
    Universal ForCausalLM wrapper with Dual-Path Design (vLLM v0.8.0, v0 engine)

    ì§€ì› ëª¨ë¸:
    - LLaMA (1, 2, 3), CodeLlama, Vicuna, Alpaca
    - Mistral, Mixtral
    - Qwen2
    - Gemma (1, 2)
    - Phi (2, 3)
    - GPT-2, GPT-NeoX
    - Falcon
    - DeepSeek (v1, v2)
    - Yi

    í•µì‹¬:
    - Path A/B ë‘˜ ë‹¤ í•­ìƒ ê³„ì‚°
    - Alphaë¡œ ê²½ë¡œ ì„ íƒ
    - ì™„ë²½í•œ CUDA Graph safety
    - v0 engine: compute_logits(hidden_states, sampling_metadata) + sample()
    """
    supports_multimodal = False
    supports_pooling = False 
    embedding_mode = False
    task = "generate"

    def __init__(
        self,
        vllm_config: VllmConfig,
        prefix: str = "",
        stage: int = 1,
    ):
        super().__init__()
        
        self.supports_lora = False
        self.embedding_mode = False
        
        config = vllm_config.model_config.hf_config
        self.config = config
        self.vllm_config = vllm_config
        
        # Get normalized model type
        self.model_type = get_model_type(config)
        
        if not hasattr(config, 'model_type'):
            config.model_type = self.model_type
        
        # Model path ê°€ì ¸ì˜¤ê¸°
        model_path = vllm_config.model_config.model
        
        # prune_log.json ë¡œë“œ
        self.prune_info = self._load_prune_log(model_path)
        
        # Stageì— ë”°ë¥¸ inactive layer indices (prune_log ê¸°ë°˜)
        inactive_indices = self._get_inactive_indices_from_prune_log(self.prune_info, stage)
        
        # Model ìƒì„± (Universal Dual-Path)
        self.model = ProgressiveModelDualPath(
            vllm_config=vllm_config,
            prefix=f"{prefix}.model" if prefix else "model",
            pruned_layer_indices=inactive_indices,
        )
        
        # LM head (vLLM v1: ParallelLMHead)
        self.lm_head = ParallelLMHead(
            config.vocab_size,
            config.hidden_size,
        )
        if config.tie_word_embeddings:
            self.lm_head = self.lm_head.tie_weights(self.model.embed_tokens)

        # vLLM v0.8.0: LogitsProcessor + Sampler
        logit_scale = getattr(config, "logit_scale", 1.0)
        self.logits_processor = LogitsProcessor(config.vocab_size,
                                                config.vocab_size,
                                                logit_scale)
        self.sampler = get_sampler()

        # Stage
        self.current_stage = stage

        # Inactive layer tracking (for weight loading)
        self.inactive_layer_indices = set(inactive_indices)

        # ì´ˆê¸° ìºì‹± ë²”ìœ„ ì„¤ì •
        max_cacheable = self._get_max_cacheable_layer()
        self.model._max_cacheable_layer = max_cacheable
        if max_cacheable is not None:
            print(f"âœ… Initial cache limit: layers 0-{max_cacheable}")
        
        print(f"\n{'='*60}")
        print(f"ProgressiveForCausalLM (Universal, vLLM v0.8.0 v0 engine)")
        print(f"Model Type: {self.model_type}")
        print(f"Model Path: {model_path}")
        print(f"Initialized at Stage {stage}")
        if self.prune_info:
            print(f"âœ… Prune log loaded from: {model_path}/prune_log.json")
            print(f"   Split B (Stage 2): {self.prune_info['split']['B']}")
            print(f"   Split C (Stage 3): {self.prune_info['split']['C']}")
        else:
            print(f"âš ï¸  Using fallback inactive layers (no prune_log.json)")
        print(f"Initially inactive layers: {sorted(inactive_indices)}")
        print(f"ğŸ¯ Dual-Path: Path A/B always computed")
        print(f"âœ… CUDA Graph safe: Topology invariant")
        print(f"{'='*60}\n")
    
    def _load_prune_log(self, model_path: str) -> Optional[dict]:
        """ëª¨ë¸ ë””ë ‰í† ë¦¬ì—ì„œ prune_log.json ë¡œë“œ"""
        import json
        import os
        
        prune_log_path = os.path.join(model_path, "prune_log.json")
        
        if not os.path.exists(prune_log_path):
            return None
        
        try:
            with open(prune_log_path, 'r') as f:
                prune_log = json.load(f)
            
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            if 'split' not in prune_log:
                print(f"âš ï¸  Warning: 'split' field not found in prune_log.json")
                return None
            
            if 'B' not in prune_log['split'] or 'C' not in prune_log['split']:
                print(f"âš ï¸  Warning: 'B' or 'C' not found in split")
                return None
            
            return prune_log
            
        except Exception as e:
            print(f"âŒ Error loading prune_log.json: {e}")
            return None
    
    def _get_inactive_indices_from_prune_log(
        self, 
        prune_info: Optional[dict], 
        stage: int
    ) -> List[int]:
        """prune_log.jsonì˜ split ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ inactive layer indices ê²°ì •"""
        # Fallback: prune_logê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        if prune_info is None:
            return self._get_inactive_indices_fallback(stage)
        
        try:
            split_b = prune_info['split']['B']
            split_c = prune_info['split']['C']
            
            if stage == 1:
                # Stage 1: B + C ëª¨ë‘ inactive
                inactive = sorted(split_b + split_c)
            elif stage == 2:
                # Stage 2: Cë§Œ inactive
                inactive = sorted(split_c)
            elif stage == 3:
                # Stage 3: ëª¨ë‘ active
                inactive = []
            else:
                raise ValueError(f"Invalid stage: {stage}. Must be 1, 2, or 3")
            
            return inactive
            
        except Exception as e:
            print(f"âŒ Error parsing prune_log: {e}")
            print(f"   Falling back to default inactive layers")
            return self._get_inactive_indices_fallback(stage)
    
    def _get_inactive_indices_fallback(self, stage: int) -> List[int]:
        """
        Fallback: prune_logê°€ ì—†ì„ ë•Œ ê¸°ë³¸ê°’
        
        ëª¨ë¸ë³„ë¡œ ë‹¤ë¥¸ ë ˆì´ì–´ ìˆ˜ë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤.
        """
        num_layers = self.config.num_hidden_layers
        
        if stage == 1:
            # Last ~25% of layers inactive
            start = int(num_layers * 0.75)
            return list(range(start, num_layers))
        elif stage == 2:
            # Last ~12% of layers inactive
            start = int(num_layers * 0.88)
            return list(range(start, num_layers))
        elif stage == 3:
            return []  # ëª¨ë‘ í™œì„±
        else:
            raise ValueError(f"Invalid stage: {stage}")
    
    def embed_input_ids(self, input_ids: torch.Tensor) -> torch.Tensor:
        """vLLM v1 ì¸í„°í˜ì´ìŠ¤: í† í° ì„ë² ë”©"""
        return self.model.embed_tokens(input_ids)

    def compute_logits(
        self,
        hidden_states: torch.Tensor,
        sampling_metadata: SamplingMetadata,
    ) -> Optional[torch.Tensor]:
        """vLLM v0.8.0 v0 engine: logits ê³„ì‚° (sampling_metadata í•„ìš”)"""
        logits = self.logits_processor(self.lm_head, hidden_states,
                                       sampling_metadata)
        return logits

    def sample(
        self,
        logits: torch.Tensor,
        sampling_metadata: SamplingMetadata,
    ) -> Optional[SamplerOutput]:
        """vLLM v0.8.0 v0 engine: sampling"""
        next_tokens = self.sampler(logits, sampling_metadata)
        return next_tokens

    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        intermediate_tensors: Optional[Any] = None,
        inputs_embeds: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """vLLM v0.8.0 v0 engine í˜¸í™˜ forward"""
        # Universal Dual-Path model forward
        hidden_states = self.model(
            input_ids=input_ids,
            positions=positions,
            intermediate_tensors=intermediate_tensors,
            inputs_embeds=inputs_embeds,
        )

        return hidden_states
    
    # ============================================================
    # Weight Loading (Universal, vLLM v0.8.0 compatible)
    # ============================================================

    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
        """
        ë²”ìš© weight loading (vLLM v0.8.0 í˜¸í™˜)
        
        ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ìë™ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì´ë¦„ íŒ¨í„´ì„ ì ìš©í•©ë‹ˆë‹¤.
        """
        print(f"\n{'='*60}")
        print(f"LOADING WEIGHTS (Universal, vLLM v0.8.0)")
        print(f"Model Type: {self.model_type}")
        print(f"{'='*60}")
        
        # Convert iterator to dict for easier handling
        checkpoint_weights = {}
        for name, tensor in weights:
            checkpoint_weights[name] = tensor
        
        print(f"Total weights in checkpoint: {len(checkpoint_weights)}")
        
        # Get weight naming pattern for this model
        weight_pattern = get_weight_pattern(self.model_type)
        print(f"Using weight pattern for: {self.model_type}")
        
        # Collect all parameters
        params_dict = dict(self.named_parameters())
        total_params = len(params_dict)
        
        loaded_keys = set()
        loaded_count = 0
        
        # Load weights
        for param_name, param in params_dict.items():
            # Option 1: Direct match
            if param_name in checkpoint_weights:
                weight_loader = getattr(param, "weight_loader",
                                       lambda p, w: p.data.copy_(w))
                weight_loader(param, checkpoint_weights[param_name])
                loaded_keys.add(param_name)
                loaded_count += 1
                continue
            
            # Option 2: Match without .layer prefix (for wrapped layers)
            alt_name = param_name.replace(".layer.", ".")
            if alt_name in checkpoint_weights:
                weight_loader = getattr(param, "weight_loader",
                                       lambda p, w: p.data.copy_(w))
                weight_loader(param, checkpoint_weights[alt_name])
                loaded_keys.add(param_name)
                loaded_count += 1
                continue
            
            # Option 3: Fused QKV weights (ë²”ìš©)
            if weight_pattern.qkv_fused_name and weight_pattern.qkv_fused_name in param_name:
                qkv_loaded = self._load_qkv_weights(
                    param, param_name, checkpoint_weights, weight_pattern
                )
                if qkv_loaded:
                    loaded_keys.add(param_name)
                    loaded_count += 1
                    continue

            # Option 4: Fused Gate-Up weights (ë²”ìš©)
            if weight_pattern.mlp_fused_name and weight_pattern.mlp_fused_name in param_name:
                mlp_loaded = self._load_mlp_weights(
                    param, param_name, checkpoint_weights, weight_pattern
                )
                if mlp_loaded:
                    loaded_keys.add(param_name)
                    loaded_count += 1
                    continue
        
        # Missing weights ì²˜ë¦¬
        missing_keys = set(params_dict.keys()) - loaded_keys
        
        if missing_keys:
            print(f"\nâš ï¸  Found {len(missing_keys)} missing weights")
            print(f"   Initializing them to ZEROS (for inactive layers)...")
            
            # Layerë³„ë¡œ ê·¸ë£¹í™”
            missing_by_layer = {}
            for key in missing_keys:
                parts = key.split('.')
                if len(parts) >= 4 and parts[0] == "model" and parts[1] == "layers":
                    try:
                        layer_idx = int(parts[2])
                        if layer_idx not in missing_by_layer:
                            missing_by_layer[layer_idx] = []
                        missing_by_layer[layer_idx].append(key)
                    except ValueError:
                        pass
            
            # Layerë³„ ì´ˆê¸°í™”
            zero_initialized = 0
            for layer_idx in sorted(missing_by_layer.keys()):
                layer_keys = missing_by_layer[layer_idx]
                
                if layer_idx in self.inactive_layer_indices:
                    # Inactive layer: 0ìœ¼ë¡œ ì´ˆê¸°í™” (ì˜ˆìƒëœ ë™ì‘)
                    print(f"   Layer {layer_idx}: Initializing {len(layer_keys)} weights to ZERO (inactive)")
                    
                    for key in layer_keys:
                        param = params_dict[key]
                        nn.init.zeros_(param)
                        zero_initialized += 1
                else:
                    # Active layerì¸ë° missing â†’ ê²½ê³ !
                    print(f"   âš ï¸  Layer {layer_idx}: Missing {len(layer_keys)} weights (ACTIVE layer!)")
                    
                    # ê·¸ë˜ë„ 0ìœ¼ë¡œ ì´ˆê¸°í™” (ì—ëŸ¬ ë°©ì§€)
                    for key in layer_keys:
                        param = params_dict[key]
                        nn.init.zeros_(param)
                        zero_initialized += 1
            
            print(f"âœ… Initialized {zero_initialized} missing weights to ZERO")
        
        print(f"\n{'='*60}")
        print(f"WEIGHT LOADING SUMMARY")
        print(f"{'='*60}")
        print(f"Total parameters:      {total_params}")
        print(f"Loaded from checkpoint: {loaded_count}")
        print(f"Initialized to zero:    {len(missing_keys)}")
        print(f"Coverage:               {loaded_count / total_params * 100:.1f}%")
        print(f"{'='*60}\n")
    
    def _load_qkv_weights(
        self,
        param,
        param_name: str,
        checkpoint_weights: dict[str, torch.Tensor],
        weight_pattern: Any,
    ) -> bool:
        """ë²”ìš© QKV weight ë¡œë”©"""
        # Build expected weight names based on pattern
        weight_names = []
        
        if ".layer." in param_name:
            checkpoint_base = param_name.replace(f".layer.self_attn.{weight_pattern.qkv_fused_name}.weight", "")
        else:
            checkpoint_base = param_name.replace(f".self_attn.{weight_pattern.qkv_fused_name}.weight", "")
        
        for proj_name in weight_pattern.qkv_weights:
            if "self_attn" in param_name:
                weight_name = f"{checkpoint_base}.self_attn.{proj_name}.weight"
            elif "attn" in param_name:
                weight_name = f"{checkpoint_base}.attn.{proj_name}.weight"
            else:
                # Fallback
                weight_name = f"{checkpoint_base}.{proj_name}.weight"
            
            weight_names.append(weight_name)
        
        # Check if all weights exist
        if all(name in checkpoint_weights for name in weight_names):
            qkv_weight = torch.cat([
                checkpoint_weights[name] for name in weight_names
            ], dim=0)
            
            weight_loader = getattr(param, "weight_loader",
                                   lambda p, w: p.data.copy_(w))
            weight_loader(param, qkv_weight)
            return True
        
        return False
    
    def _load_mlp_weights(
        self,
        param,
        param_name: str,
        checkpoint_weights: Dict[str, torch.Tensor],
        weight_pattern: Any,
    ) -> bool:
        """ë²”ìš© MLP weight ë¡œë”©"""
        if not weight_pattern.mlp_gate_up:
            return False
        
        # Build expected weight names based on pattern
        weight_names = []
        
        if ".layer." in param_name:
            checkpoint_base = param_name.replace(f".layer.mlp.{weight_pattern.mlp_fused_name}.weight", "")
        else:
            checkpoint_base = param_name.replace(f".mlp.{weight_pattern.mlp_fused_name}.weight", "")
        
        for proj_name in weight_pattern.mlp_gate_up:
            weight_name = f"{checkpoint_base}.mlp.{proj_name}.weight"
            weight_names.append(weight_name)
        
        # Check if all weights exist
        if all(name in checkpoint_weights for name in weight_names):
            mlp_weight = torch.cat([
                checkpoint_weights[name] for name in weight_names
            ], dim=0)
            
            weight_loader = getattr(param, "weight_loader",
                                   lambda p, w: p.data.copy_(w))
            weight_loader(param, mlp_weight)
            return True
        
        return False
    
    # ============================================================
    # Progressive Recovery (Alpha Gating)
    # ============================================================
    
    def _get_b_indices(self) -> List[int]:
        if self.prune_info:
            return list(self.prune_info['split']['B'])
        num = self.config.num_hidden_layers
        return list(range(int(num * 0.75), int(num * 0.88)))

    def _get_c_indices(self) -> List[int]:
        if self.prune_info:
            return list(self.prune_info['split']['C'])
        num = self.config.num_hidden_layers
        return list(range(int(num * 0.88), num))

    def get_recompute_boundary(self, newly_activated_indices: List[int]) -> Optional[int]:
        """
        Partial KV recomputationì„ ìœ„í•œ boundary layer ê³„ì‚°.

        Returns:
            boundary layer index (ì´ ë ˆì´ì–´ë¶€í„° full forward í•„ìš”)
            Noneì´ë©´ partial recompute ë¶ˆê°€ (ì „ì²´ recompute)
        """
        if not newly_activated_indices:
            return None

        # Boundary = ìƒˆë¡œ í™œì„±í™”ëœ ë ˆì´ì–´ ì¤‘ ìµœì†Ÿê°’
        # ì´ ë ˆì´ì–´ë¶€í„° hidden statesê°€ ë³€ê²½ë˜ë¯€ë¡œ full forward í•„ìš”
        boundary = min(newly_activated_indices)

        if boundary <= 0:
            return None  # ì²« ë ˆì´ì–´ë¶€í„° ë³€ê²½ â†’ full recompute

        return boundary

    def _get_max_cacheable_layer(self) -> Optional[int]:
        """
        í˜„ì¬ stageì—ì„œ ìºì‹±í•  ìµœëŒ€ ë ˆì´ì–´ ì¸ë±ìŠ¤ ë°˜í™˜.
        ë‹¤ìŒ stageì˜ boundary-1ì„ ë°˜í™˜.
        """
        if self.current_stage == 1:
            # Stage 1: Stage 2 boundary-1ê¹Œì§€ë§Œ ìºì‹±
            b_indices = self._get_b_indices()
            if b_indices:
                return min(b_indices) - 1
        elif self.current_stage == 2:
            # Stage 2: Stage 3 boundary-1ê¹Œì§€ë§Œ ìºì‹±
            c_indices = self._get_c_indices()
            if c_indices:
                return min(c_indices) - 1
        # Stage 3 or unknown: ëª¨ë“  ë ˆì´ì–´ ìºì‹±
        return None

    def prefetch_stage2(self, checkpoint_path: str) -> None:
        """Stage 2 weightsë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ CPUì— ë¯¸ë¦¬ ë¡œë“œ. Stage 1 ì„œë¹™ ì‹œì‘ ì§í›„ í˜¸ì¶œ."""
        self.model.prefetch_weights(checkpoint_path, self._get_b_indices())

    def prefetch_stage3(self, checkpoint_path: str) -> None:
        """Stage 3 weightsë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ CPUì— ë¯¸ë¦¬ ë¡œë“œ. Stage 2 ì„œë¹™ ì‹œì‘ ì§í›„ í˜¸ì¶œ."""
        self.model.prefetch_weights(checkpoint_path, self._get_c_indices())

    def advance_to_stage2_instant(self, wait_if_needed: bool = True) -> bool:
        """
        prefetchëœ weightsë¡œ ì¦‰ê° Stage 2 ì „í™˜.
        ë””ìŠ¤í¬ I/O ì—†ì´ GPU copy + alpha ë³€ê²½ë§Œ ì‹¤í–‰.
        prefetch_stage2()ê°€ ë¨¼ì € í˜¸ì¶œë˜ì–´ ìˆì–´ì•¼ í•¨.

        Partial KV recomputation: B ë ˆì´ì–´ë“¤ì´ í™œì„±í™”ë˜ë¯€ë¡œ boundary ì„¤ì •
        """
        b_indices = self._get_b_indices()

        success = self.model.activate_layers_instant(
            b_indices,
            wait_if_needed=wait_if_needed,
        )

        if success:
            self.current_stage = 2
            self.inactive_layer_indices = set(self._get_c_indices())

            # Partial KV recomputation ì„¤ì •
            boundary = self.get_recompute_boundary(b_indices)
            if boundary is not None:
                self.model.set_partial_recompute(boundary)
                print(f"[Stage2] Partial recompute enabled: boundary={boundary}")

            # ìºì‹± ë²”ìœ„ ì„¤ì • (Stage 3 boundary-1ê¹Œì§€)
            max_cacheable = self._get_max_cacheable_layer()
            self.model._max_cacheable_layer = max_cacheable
            if max_cacheable is not None:
                print(f"[Stage2] Caching layers 0-{max_cacheable} (Stage 3 ì¤€ë¹„)")

            print(f"\n{'='*80}")
            print(f"NOW AT STAGE 2 (instant)")
            print(f"{'='*80}\n")
            self.print_status()

        return success

    def advance_to_stage3_instant(self, wait_if_needed: bool = True) -> bool:
        """
        prefetchëœ weightsë¡œ ì¦‰ê° Stage 3 ì „í™˜.
        prefetch_stage3()ê°€ ë¨¼ì € í˜¸ì¶œë˜ì–´ ìˆì–´ì•¼ í•¨.

        Partial KV recomputation: C ë ˆì´ì–´ë“¤ì´ í™œì„±í™”ë˜ë¯€ë¡œ boundary ì„¤ì •
        """
        c_indices = self._get_c_indices()

        success = self.model.activate_layers_instant(
            c_indices,
            wait_if_needed=wait_if_needed,
        )

        if success:
            self.current_stage = 3
            self.inactive_layer_indices = set()

            # Partial KV recomputation ì„¤ì •
            boundary = self.get_recompute_boundary(c_indices)
            if boundary is not None:
                self.model.set_partial_recompute(boundary)
                print(f"[Stage3] Partial recompute enabled: boundary={boundary}")

            # ìºì‹± ë²”ìœ„ ì„¤ì • (Stage 3ëŠ” ëª¨ë“  ë ˆì´ì–´)
            self.model._max_cacheable_layer = None
            print(f"[Stage3] Caching all layers (final stage)")

            print(f"\n{'='*80}")
            print(f"NOW AT STAGE 3 - FULL MODEL (instant)")
            print(f"{'='*80}\n")
            self.print_status()

        return success

    def is_stage2_ready(self) -> bool:
        """Stage 2 prefetch ì™„ë£Œ ì—¬ë¶€ (non-blocking í™•ì¸ìš©)"""
        return self.model.is_prefetch_ready()

    def is_prefetch_ready(self) -> bool:
        """ìµœê·¼ prefetch ì™„ë£Œ ì—¬ë¶€ (Stage 2/3 ê³µí†µ)"""
        return self.model.is_prefetch_ready()

    def wait_for_prefetch(self, timeout_s: Optional[float] = None) -> bool:
        """ìµœê·¼ prefetch ì™„ë£Œê¹Œì§€ ëŒ€ê¸°"""
        return self.model.wait_for_prefetch(timeout_s=timeout_s)

    def get_prefetch_status(self) -> dict:
        """ìµœê·¼ prefetch ìƒíƒœ ë°˜í™˜"""
        return self.model.get_prefetch_status()

    def advance_to_stage2(
        self,
        layer_b_checkpoint: str,
        adapter_ab_path: Optional[str] = None,
    ) -> None:
        """Stage 1 â†’ Stage 2 (prune_log ê¸°ë°˜)"""
        print("\n" + "="*80)
        print(f"ADVANCING TO STAGE 2 (Universal, {self.model_type})")
        print("="*80)
        
        # prune_logì—ì„œ B ë ˆì´ì–´ ê°€ì ¸ì˜¤ê¸°
        if self.prune_info is None:
            print("âš ï¸  Warning: No prune_log available. Using fallback.")
            num_layers = self.config.num_hidden_layers
            start = int(num_layers * 0.75)
            end = int(num_layers * 0.88)
            activate_indices = list(range(start, end))
        else:
            activate_indices = self.prune_info['split']['B']
            print(f"Activating layers from prune_log: {activate_indices}")
        
        # B ë ˆì´ì–´ í™œì„±í™”
        self.model.activate_layers(
            layer_indices=activate_indices,
            checkpoint_path=layer_b_checkpoint,
        )
        
        # Adapter (optional)
        if adapter_ab_path:
            print(f"Loading AB adapter from: {adapter_ab_path}")
        
        # Stage ì—…ë°ì´íŠ¸
        self.current_stage = 2
        
        # Inactive layers ì—…ë°ì´íŠ¸ (Cë§Œ)
        if self.prune_info:
            self.inactive_layer_indices = set(self.prune_info['split']['C'])
        else:
            num_layers = self.config.num_hidden_layers
            start = int(num_layers * 0.88)
            self.inactive_layer_indices = set(range(start, num_layers))
        
        print(f"\n{'='*80}")
        print(f"NOW AT STAGE 2")
        print(f"{'='*80}\n")
        
        self.print_status()
    
    def advance_to_stage3(
        self,
        layer_c_checkpoint: str,
        remove_adapter: bool = True,
    ) -> None:
        """Stage 2 â†’ Stage 3 (prune_log ê¸°ë°˜)"""
        print("\n" + "="*80)
        print(f"ADVANCING TO STAGE 3 (Universal, {self.model_type})")
        print("="*80)
        
        # prune_logì—ì„œ C ë ˆì´ì–´ ê°€ì ¸ì˜¤ê¸°
        if self.prune_info is None:
            print("âš ï¸  Warning: No prune_log available. Using fallback.")
            num_layers = self.config.num_hidden_layers
            start = int(num_layers * 0.88)
            activate_indices = list(range(start, num_layers))
        else:
            activate_indices = self.prune_info['split']['C']
            print(f"Activating layers from prune_log: {activate_indices}")
        
        # C ë ˆì´ì–´ í™œì„±í™”
        self.model.activate_layers(
            layer_indices=activate_indices,
            checkpoint_path=layer_c_checkpoint,
        )
        
        # Adapter ì œê±°
        if remove_adapter:
            print("Removing all adapters...")
        
        # Stage ì—…ë°ì´íŠ¸
        self.current_stage = 3
        self.inactive_layer_indices = set()  # ëª¨ë‘ í™œì„±
        
        print(f"\n{'='*80}")
        print(f"NOW AT STAGE 3 - FULL MODEL")
        print(f"{'='*80}\n")
        
        self.print_status()
    
    # ============================================================
    # Status and Info Methods
    # ============================================================
    
    def print_status(self) -> None:
        """í˜„ì¬ ëª¨ë¸ ìƒíƒœ ì¶œë ¥"""
        self.model.print_layer_status()
        
        print(f"Current Stage: {self.current_stage}")
        print(f"Model Type: {self.model_type}")
        
        report = self.model.verify_recovery()
        print(f"Activation Progress: {report['activation_progress']}")
        
        adapter_info = self.model.get_adapter_info()
        print(f"Current Adapter: {adapter_info['current_adapter'] or 'None'}")
        print()
    
    def get_stage_info(self) -> dict:
        """í˜„ì¬ stage ì •ë³´ ë°˜í™˜ (prune_info í¬í•¨)"""
        report = self.model.verify_recovery()
        adapter_info = self.model.get_adapter_info()
        
        return {
            "stage": self.current_stage,
            "model_type": self.model_type,
            "active_layers": report["active_layers"],
            "inactive_layers": report["inactive_layers"],
            "activation_progress": report["activation_progress"],
            "current_adapter": adapter_info["current_adapter"],
            "inactive_layer_indices": report["inactive_layer_indices"],
            "prune_info": self.prune_info,
        }
    
    def get_layer_alphas(self) -> List[float]:
        """ëª¨ë“  ë ˆì´ì–´ì˜ alpha ê°’ ë°˜í™˜"""
        alphas = []
        for layer in self.model.layers:
            if hasattr(layer, 'get_alpha_value'):
                alphas.append(layer.get_alpha_value())
            else:
                alphas.append(1.0)  # Normal layer
        return alphas
    
    def set_layer_alpha(self, layer_idx: int, alpha: float):
        """íŠ¹ì • ë ˆì´ì–´ì˜ alpha ê°’ ì§ì ‘ ì„¤ì •"""
        if layer_idx >= len(self.model.layers):
            raise ValueError(f"Invalid layer index: {layer_idx}")
        
        layer = self.model.layers[layer_idx]
        if hasattr(layer, 'set_alpha'):
            layer.set_alpha(alpha) 
            print(f"Layer {layer_idx} alpha set to {alpha}")
        else:
            print(f"Layer {layer_idx} is not an AlphaGatedLayer")
