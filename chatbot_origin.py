#!/usr/bin/env python3
"""
Progressive Serving Chatbot - Origin (Full KV Recompute on Stage Transition)
=============================================================================

Interactive chatbot with progressive model serving (vLLM v0 engine).
Stage transitions on user command (/stage2, /stage3).

Uses origin_progressive_serve (partial recompute ìµœì í™” ì—†ìŒ).

**ë™ì‘ ë°©ì‹:**
- ì¼ë°˜ ëŒ€í™”: prefix cachingìœ¼ë¡œ KV cache í„´ ê°„ ì¬ì‚¬ìš© (ë¹ ë¦„)
- Stage ì „í™˜: ë§¥ë½(conversation history) ìœ ì§€ + KV cache ì™„ì „ ì´ˆê¸°í™”
  â†’ ë‹¤ìŒ generate() í˜¸ì¶œ ì‹œ vLLMì´ ìë™ìœ¼ë¡œ ì „ì²´ prefill ì‹¤í–‰
  â†’ ìƒˆ stage ê°€ì¤‘ì¹˜ë¡œ KV cache ìë™ ì¬êµ¬ì¶• (ë³„ë„ ì½”ë“œ ë¶ˆí•„ìš”)

**chatbot_partial_cache.py ì™€ì˜ ì°¨ì´:**
- progressive_serve â†’ origin_progressive_serve ì‚¬ìš©
- KV Snapshot / Partial recompute ì—†ìŒ
- Stage ì „í™˜ ì‹œ reset_prefix_cache()ë§Œ í˜¸ì¶œ â†’ ë‹¤ìŒ turnì—ì„œ full prefill

Usage:
  python chatbot_origin.py --model llama
  python chatbot_origin.py --model mistral

  Commands during chat:
    /stage2  - Transition to Stage 2 (KV cache cleared, full recompute on next turn)
    /stage3  - Transition to Stage 3 (KV cache cleared, full recompute on next turn)
    /status  - Show model status
    /reset   - Reset conversation
    /quit    - Exit
"""

import os
import sys
import json
import time
import argparse

# vLLM v0 ì—”ì§„ ê°•ì œ ì‚¬ìš© (ëª¨ë¸ ì§ì ‘ ì ‘ê·¼ í•„ìš”)
os.environ["VLLM_USE_V1"] = "0"

import torch
from vllm import LLM, SamplingParams
from vllm.model_executor.models.registry import ModelRegistry

# origin_progressive_serve: partial recompute ì—†ëŠ” ì›ë³¸ êµ¬í˜„
_ORIGIN_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "origin_progressive_serve")
sys.path.insert(0, _ORIGIN_DIR)

# progressive_model_dual_path.py ë‚´ë¶€ì— hardcoded sys.pathê°€ ìˆì–´ì„œ
# model_configë¥¼ ë¨¼ì € ìºì‹±í•´ë‘ë©´ í•´ë‹¹ ê²½ë¡œë¥¼ ë¬´ì‹œí•˜ê³  ì˜¬ë°”ë¥¸ ë²„ì „ì´ ì‚¬ìš©ë¨
import model_config  # noqa: F401  (origin_progressive_serve/model_config.py)
from progressive_for_causal_lm import ProgressiveForCausalLM


# ============================================================================
# ëª¨ë¸ ì„¤ì •
# ============================================================================

MODELS = {
    "llama": {
        "progressive_path":   "/acpl-ssd30/7b_results/pruning/A",
        "stage_b_checkpoint": "/acpl-ssd30/7b_results/pruning/checkpoints/stage2_layers_B.safetensors",
        "stage_c_checkpoint": "/acpl-ssd30/7b_results/pruning/checkpoints/stage3_layers_C.safetensors",
    },
    "mistral": {
        "progressive_path":   "/home/devewha/entropy_routing/25_mistral_results/pruning/A",
        "stage_b_checkpoint": "/acpl-ssd30/25_mistral_results/pruning/bundles/stage2_layers_B.safetensors",
        "stage_c_checkpoint": "/acpl-ssd30/25_mistral_results/pruning/bundles/stage3_layers_C.safetensors",
    },
}


# ============================================================================
# Chatbot - Full KV Recompute on Stage Transition
# ============================================================================

class ProgressiveChatbotOrigin:
    """
    Origin progressive serving ê¸°ë°˜ ëŒ€í™”í˜• ì±—ë´‡.

    í•µì‹¬ ê¸°ëŠ¥:
    - ì¼ë°˜ ëŒ€í™”: prefix cachingìœ¼ë¡œ KV cache ì¬ì‚¬ìš© (ë¹ ë¦„)
    - Stage ì „í™˜: ë§¥ë½ ìœ ì§€ + KV cache ì™„ì „ ì´ˆê¸°í™”
      â†’ ë‹¤ìŒ generate() í˜¸ì¶œ ì‹œ vLLMì´ ìƒˆ stage ê°€ì¤‘ì¹˜ë¡œ KV cache ìë™ ì¬êµ¬ì¶•
    - Partial recompute / KV snapshot ì—†ìŒ (origin ë²„ì „)
    """

    def __init__(self, model_name: str):
        self.model_name = model_name
        self.config = MODELS[model_name]
        self.current_stage = 1
        self.conversation = []  # [{"role": "user"/"assistant", "content": "..."}]

        model_path = self.config["progressive_path"]
        with open(os.path.join(model_path, "config.json")) as f:
            arch = json.load(f)["architectures"][0]
        ModelRegistry.register_model(arch, ProgressiveForCausalLM)
        print(f"  Registered ProgressiveForCausalLM as: {arch}")

        print(f"\n  Loading {model_name} Stage 1...")
        self.llm = LLM(
            model=model_path,
            trust_remote_code=True,
            gpu_memory_utilization=0.4,
            max_model_len=2048,
            enforce_eager=False,
            # ì¼ë°˜ ëŒ€í™” ì¤‘ KV cache ì¬ì‚¬ìš© (prefix caching)
            # Stage ì „í™˜ ì‹œ reset_prefix_cache()ë¡œ ì´ˆê¸°í™”
            enable_prefix_caching=True,
        )

        self.model = self._get_model_handle()
        self.tokenizer = self.llm.get_tokenizer()
        self.sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=512,
        )

        print(f"  âœ… Prefix caching enabled (KV cache persists between turns)")
        print(f"  âœ… Stage transitions clear KV cache â†’ full recompute on next turn")

    def _get_model_handle(self):
        """v0 ì—”ì§„ì—ì„œ progressive model ê°ì²´ ê°€ì ¸ì˜¤ê¸°"""
        engine = self.llm.llm_engine
        if hasattr(engine, "engine_core"):
            raise RuntimeError(
                "V1 engine detected. This script is v0-only. Use VLLM_USE_V1=0."
            )
        try:
            return engine.model_executor.driver_worker.worker.model_runner.model
        except AttributeError as exc:
            raise RuntimeError("Could not resolve v0 model handle path.") from exc

    # ----------------------------------------------------------------
    # í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
    # ----------------------------------------------------------------
    def _build_prompt(self) -> str:
        """ëŒ€í™” ê¸°ë¡ â†’ ì „ì²´ í”„ë¡¬í”„íŠ¸ (chat template ìš°ì„ )"""
        if hasattr(self.tokenizer, "apply_chat_template"):
            try:
                return self.tokenizer.apply_chat_template(
                    self.conversation,
                    tokenize=False,
                    add_generation_prompt=True,
                )
            except Exception:
                pass
        # Fallback: ë‹¨ìˆœ í¬ë§·
        prompt = ""
        for msg in self.conversation:
            prefix = "User: " if msg["role"] == "user" else "Assistant: "
            prompt += prefix + msg["content"] + "\n"
        return prompt + "Assistant: "

    # ----------------------------------------------------------------
    # ì±„íŒ…
    # ----------------------------------------------------------------
    def chat(self, user_input: str) -> str:
        """
        ì‚¬ìš©ì ì…ë ¥ â†’ ì‘ë‹µ ìƒì„±.

        Prefix caching í™œì„±í™”ë¡œ ì´ì „ ëŒ€í™”ì˜ KV cacheê°€ ì¬ì‚¬ìš©ë©ë‹ˆë‹¤.
        Stage ì „í™˜ ì§í›„ ì²« turnì—ëŠ” cache miss â†’ vLLMì´ full prefill ìë™ ì‹¤í–‰.
        """
        self.conversation.append({"role": "user", "content": user_input})
        prompt = self._build_prompt()

        token_ids = self.tokenizer.encode(prompt)
        if len(token_ids) > 1800:
            print(f"  [Warning] Conversation length ({len(token_ids)} tokens) "
                  f"approaching limit. Consider /reset.")

        outputs = self.llm.generate([prompt], self.sampling_params)
        response = outputs[0].outputs[0].text.strip()
        self.conversation.append({"role": "assistant", "content": response})
        return response

    # ----------------------------------------------------------------
    # KV Cache ì´ˆê¸°í™”
    # ----------------------------------------------------------------
    def _clear_kv_cache(self) -> None:
        """
        Stage ì „í™˜ í›„ stale KV cache ì „ì²´ ì´ˆê¸°í™”.

        Stage ì „í™˜ìœ¼ë¡œ model weightsê°€ ë³€ê²½ë˜ë¯€ë¡œ ê¸°ì¡´ KV blocksëŠ” ë¬´íš¨.
        reset_prefix_cache()ë¡œ ëª¨ë“  cached blocksë¥¼ evictí•œ ë’¤,
        ë‹¤ìŒ generate() í˜¸ì¶œ ì‹œ vLLMì´ ìƒˆ stage weightsë¡œ full prefillì„
        ìë™ ì‹¤í–‰í•˜ì—¬ KV cacheë¥¼ ì¬êµ¬ì¶•í•©ë‹ˆë‹¤.

        ë³„ë„ì˜ ì¬ê³„ì‚° ì½”ë“œê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤ â€” ë‹¤ìŒ turnì—ì„œ ìë™ ì²˜ë¦¬ë©ë‹ˆë‹¤.
        """
        try:
            success = self.llm.reset_prefix_cache()
            if success:
                print(f"  [KVCache] âœ… All prefix cache blocks evicted")
                print(f"  [KVCache]    Next turn will run full prefill "
                      f"with new Stage {self.current_stage} weights")
            else:
                print(f"  [KVCache] âš ï¸  reset_prefix_cache() returned False "
                      f"(blocks may still be in use)")
        except Exception as e:
            print(f"  [KVCache] âš ï¸  Could not clear prefix cache: {e}")

    # ----------------------------------------------------------------
    # Stage ì „í™˜
    # ----------------------------------------------------------------
    def advance_to_stage2(self) -> bool:
        """
        Stage 1 â†’ Stage 2 ì „í™˜.

        ë™ì‘:
        1. prefetch_stage2(): checkpointë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ CPUì— ë¡œë“œ
        2. wait_for_prefetch(): ì™„ë£Œ ëŒ€ê¸°
        3. advance_to_stage2_instant(): GPU weight copy + alpha ë³€ê²½
        4. _clear_kv_cache(): stale KV blocks í‡´ì¶œ
           â†’ ë‹¤ìŒ turnì—ì„œ vLLMì´ full prefill ìë™ ì‹¤í–‰
        """
        if self.current_stage >= 2:
            print("  Already at Stage 2 or higher.")
            return False

        stage_b_path = self.config.get("stage_b_checkpoint")
        if not stage_b_path or not os.path.exists(stage_b_path):
            print(f"  Stage B checkpoint not found: {stage_b_path}")
            return False

        print("  [Stage 1 â†’ 2] Prefetching B layers...")
        t0 = time.time()
        self.model.prefetch_stage2(stage_b_path)

        ready = self.model.wait_for_prefetch(timeout_s=120.0)
        if not ready:
            print("  Stage 2 prefetch failed or timed out.")
            return False

        transitioned = self.model.advance_to_stage2_instant(wait_if_needed=False)
        if not transitioned:
            print("  Stage 2 instant transition failed.")
            return False

        self.current_stage = 2
        elapsed = time.time() - t0

        stage_info = self.model.get_stage_info()
        print(f"  âœ… Stage 2 transition complete ({elapsed:.2f}s)")
        print(f"  Active layers: {len(stage_info['active_layers'])}, "
              f"Progress: {stage_info['activation_progress']}")

        # KV cache ì´ˆê¸°í™” (ë‹¤ìŒ turnì—ì„œ ìë™ full prefill)
        self._clear_kv_cache()
        return True

    def advance_to_stage3(self) -> bool:
        """
        Stage 2 â†’ Stage 3 ì „í™˜.

        ë™ì‘:
        1. prefetch_stage3(): checkpointë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ CPUì— ë¡œë“œ
        2. wait_for_prefetch(): ì™„ë£Œ ëŒ€ê¸°
        3. advance_to_stage3_instant(): GPU weight copy + alpha ë³€ê²½
        4. _clear_kv_cache(): stale KV blocks í‡´ì¶œ
           â†’ ë‹¤ìŒ turnì—ì„œ vLLMì´ full prefill ìë™ ì‹¤í–‰
        """
        if self.current_stage < 2:
            print("  Must be at Stage 2 first. Use /stage2.")
            return False
        if self.current_stage >= 3:
            print("  Already at Stage 3.")
            return False

        stage_c_path = self.config.get("stage_c_checkpoint")
        if not stage_c_path or not os.path.exists(stage_c_path):
            print(f"  Stage C checkpoint not found: {stage_c_path}")
            return False

        print("  [Stage 2 â†’ 3] Prefetching C layers...")
        t0 = time.time()
        self.model.prefetch_stage3(stage_c_path)

        ready = self.model.wait_for_prefetch(timeout_s=120.0)
        if not ready:
            print("  Stage 3 prefetch failed or timed out.")
            return False

        transitioned = self.model.advance_to_stage3_instant(wait_if_needed=False)
        if not transitioned:
            print("  Stage 3 instant transition failed.")
            return False

        self.current_stage = 3
        elapsed = time.time() - t0

        stage_info = self.model.get_stage_info()
        print(f"  âœ… Stage 3 transition complete ({elapsed:.2f}s)")
        print(f"  Active layers: {len(stage_info['active_layers'])}, "
              f"Progress: {stage_info['activation_progress']}")

        # KV cache ì´ˆê¸°í™”
        self._clear_kv_cache()
        return True

    # ----------------------------------------------------------------
    # ìƒíƒœ / ë¦¬ì…‹
    # ----------------------------------------------------------------
    def reset_conversation(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.conversation = []
        print("  Conversation reset.")

    def print_status(self):
        """í˜„ì¬ ìƒíƒœ ì¶œë ¥"""
        stage_info = self.model.get_stage_info()
        print(f"\n  {'='*50}")
        print(f"  Model:    {self.model_name}")
        print(f"  Stage:    {self.current_stage}")
        print(f"  Active:   {len(stage_info['active_layers'])} layers")
        print(f"  Inactive: {len(stage_info['inactive_layers'])} layers")
        print(f"  Progress: {stage_info['activation_progress']}")
        print(f"  Turns:    {len(self.conversation) // 2}")
        print(f"  GPU Mem:  {torch.cuda.memory_allocated() / (1024**3):.2f} GB")
        print(f"  {'='*50}")


# ============================================================================
# Main
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Progressive Serving Chatbot (Origin / Full KV Recompute)"
    )
    parser.add_argument(
        "--model",
        type=str,
        choices=list(MODELS.keys()),
        default="llama",
        help="Model to use (default: llama)",
    )
    args = parser.parse_args()

    print("\n" + "=" * 60)
    print("Progressive Serving Chatbot  [Origin / Full Recompute]")
    print(f"  Model: {args.model}")
    print(f"  GPU:   {torch.cuda.get_device_name(0)}")
    print("=" * 60)

    chatbot = ProgressiveChatbotOrigin(args.model)

    print(f"\n{'='*60}")
    print(f"  Ready! (Stage {chatbot.current_stage})")
    print(f"  Commands: /stage2, /stage3, /status, /reset, /quit")
    print(f"  âœ… KV cache reused between turns (prefix caching)")
    print(f"  ğŸ”„ Stage transition â†’ KV cache cleared â†’ auto full recompute next turn")
    print(f"{'='*60}\n")

    while True:
        try:
            user_input = input(f"You [Stage {chatbot.current_stage}]: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nBye!")
            break

        if not user_input:
            continue

        if user_input == "/quit":
            print("Bye!")
            break
        elif user_input == "/stage2":
            chatbot.advance_to_stage2()
            continue
        elif user_input == "/stage3":
            chatbot.advance_to_stage3()
            continue
        elif user_input == "/status":
            chatbot.print_status()
            continue
        elif user_input == "/reset":
            chatbot.reset_conversation()
            continue

        t0 = time.time()
        response = chatbot.chat(user_input)
        elapsed = time.time() - t0

        print(f"Assistant [Stage {chatbot.current_stage}] ({elapsed:.1f}s): {response}\n")


if __name__ == "__main__":
    main()
