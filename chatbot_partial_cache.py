#!/usr/bin/env python3
"""
Progressive Serving Chatbot - Partial KV Cache Recomputation
=============================================================

Interactive chatbot with progressive model serving (vLLM v0 engine).
Stage transitions on user command (/stage2, /stage3).

**í•µì‹¬ ì°¨ì´ì  (vs chatbot_full_cache.py):**
- KV Cacheë¥¼ í„´ ì‚¬ì´ì— ìœ ì§€ (ì¬ì´ˆê¸°í™” ì•ˆ í•¨)
- Stage ì „í™˜ ì‹œì—ë§Œ ë¶€ë¶„ì  KV Cache ì¬ê³„ì‚°:
  * Boundary ì´ì „ ë ˆì´ì–´: ì™„ì „ ìŠ¤í‚µ (ê°€ì¤‘ì¹˜ ë¶ˆë³€ â†’ KV cache ê·¸ëŒ€ë¡œ ìœ íš¨)
  * Boundary ì´í›„ ë ˆì´ì–´: full forward (ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ë¡œ ì¬ê³„ì‚°)
- Hidden states CPU ìºì‹±ìœ¼ë¡œ GPU ì—°ì‚° ìµœì†Œí™”
- CUDA Graph ì¬ìº¡ì²˜ ìµœì†Œí™” (prefillì—ì„œë§Œ partial recompute)

Usage:
  python chatbot_partial_cache.py --model llama
  python chatbot_partial_cache.py --model mistral

  Commands during chat:
    /stage2  - Transition to Stage 2 (partial KV recomputation)
    /stage3  - Transition to Stage 3 (partial KV recomputation)
    /status  - Show model status
    /reset   - Reset conversation
    /quit    - Exit
"""

import os
import sys
import json
import time
import argparse

# vLLM v0 ì—”ì§„ ê°•ì œ ì‚¬ìš© (ëª¨ë¸ ì§ì ‘ ì ‘ê·¼ í•„ìš”)
os.environ["VLLM_USE_V1"] = "0"

import torch
from vllm import LLM, SamplingParams
from vllm.model_executor.models.registry import ModelRegistry

# Progressive model
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), "progressive_serve"))
from progressive_for_causal_lm import ProgressiveForCausalLM


# ============================================================================
# ëª¨ë¸ ì„¤ì • (02_universal.py ë™ì¼)
# ============================================================================

MODELS = {
    "llama": {
        "progressive_path": "/acpl-ssd30/7b_results/pruning/A",
        "stage_b_checkpoint": "/acpl-ssd30/7b_results/pruning/checkpoints/stage2_layers_B.safetensors",
        "stage_c_checkpoint": "/acpl-ssd30/7b_results/pruning/checkpoints/stage3_layers_C.safetensors",
    },
    "mistral": {
        "progressive_path": "/home/devewha/entropy_routing/25_mistral_results/pruning/A",
        "stage_b_checkpoint": "/acpl-ssd30/25_mistral_results/pruning/bundles/stage2_layers_B.safetensors",
        "stage_c_checkpoint": "/acpl-ssd30/25_mistral_results/pruning/bundles/stage3_layers_C.safetensors",
    },
}


# ============================================================================
# Chatbot with Partial KV Cache Recomputation
# ============================================================================

class ProgressiveChatbotPartial:
    """
    Progressive Serving ê¸°ë°˜ ëŒ€í™”í˜• ì±—ë´‡ (Partial KV Cache Recomputation)

    í•µì‹¬ ê¸°ëŠ¥:
    - KV Cache í„´ ê°„ ìœ ì§€ (vLLM KV ë¸”ë¡ ì¬ì‚¬ìš©)
    - Stage ì „í™˜ ì‹œ ë¶€ë¶„ ì¬ê³„ì‚°:
      * Unchanged layers (< boundary): ì™„ì „ ìŠ¤í‚µ (GPU ì—°ì‚° ì—†ìŒ, KV cache ìœ íš¨)
      * Changed layers (>= boundary): full forward (ìƒˆ ê°€ì¤‘ì¹˜ë¡œ KV ì¬ê³„ì‚°)
    - Hidden states CPU ìºì‹±ìœ¼ë¡œ GPU ì—°ì‚° ìµœì†Œí™”
    """

    def __init__(self, model_name: str):
        self.model_name = model_name
        self.config = MODELS[model_name]
        self.current_stage = 1
        self.conversation = []  # [{"role": "user"/"assistant", "content": "..."}]

        # config.jsonì—ì„œ ì•„í‚¤í…ì²˜ ì½ê¸° â†’ ë“±ë¡
        model_path = self.config["progressive_path"]
        with open(os.path.join(model_path, "config.json")) as f:
            arch = json.load(f)["architectures"][0]
        ModelRegistry.register_model(arch, ProgressiveForCausalLM)
        print(f"  Registered ProgressiveForCausalLM as: {arch}")

        # LLM ìƒì„±
        print(f"\n  Loading {model_name} Stage 1...")
        self.llm = LLM(
            model=model_path,
            trust_remote_code=True,
            gpu_memory_utilization=0.4,
            max_model_len=2048,
            # ğŸ”¥ enforce_eager=False: CUDA graph í™œì„±í™”
            # Persistent GPU buffer + index_copy_()ê°€ CUDA graphì— ìº¡ì²˜ë˜ì–´
            # decode phaseì—ì„œë„ hidden statesê°€ ìë™ ëˆ„ì ë¨
            enforce_eager=False,
            # Prefix caching í™œì„±í™” (KV cache í„´ ê°„ ìœ ì§€)
            enable_prefix_caching=True,
        )

        # v0 ì—”ì§„ ëª¨ë¸ í•¸ë“¤ (02_universal.py ë™ì¼)
        self.model = self._get_model_handle()

        # ğŸ”¥ Persistent GPU buffer: warmup ì¤‘ ê¸°ë¡ëœ ì“°ë ˆê¸°ê°’ ì œê±°
        # CUDA graph ìº¡ì²˜ í›„ bufferëŠ” ì´ë¯¸ í• ë‹¹ë˜ì–´ ìˆìŒ â†’ zero_()ë¡œ ì´ˆê¸°í™”ë§Œ
        if hasattr(self.model, 'model') and hasattr(self.model.model, 'clear_persistent_buffers'):
            self.model.model.clear_persistent_buffers()
            print(f"  âœ… Persistent GPU buffers cleared (warmup data removed)")

        # í† í¬ë‚˜ì´ì € ìºì‹œ
        self.tokenizer = self.llm.get_tokenizer()

        # Sampling params
        self.sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=512,
        )

        print(f"  âœ… Partial KV Cache Recomputation enabled")
        print(f"  âœ… Prefix caching enabled (KV cache persists between turns)")

    def _get_model_handle(self):
        """v0 ì—”ì§„ì—ì„œ progressive model ê°ì²´ ê°€ì ¸ì˜¤ê¸° (02_universal.py ë™ì¼)"""
        engine = self.llm.llm_engine
        if hasattr(engine, "engine_core"):
            raise RuntimeError(
                "V1 engine detected. This script is v0-only. "
                "Use VLLM_USE_V1=0."
            )
        try:
            return engine.model_executor.driver_worker.worker.model_runner.model
        except AttributeError as exc:
            raise RuntimeError(
                "Could not resolve v0 model handle path."
            ) from exc

    # ----------------------------------------------------------------
    # í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
    # ----------------------------------------------------------------
    def _build_prompt(self) -> str:
        """ëŒ€í™” ê¸°ë¡ â†’ ì „ì²´ í”„ë¡¬í”„íŠ¸ ìƒì„± (chat template ì‚¬ìš©)"""
        # chat_template ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©
        if hasattr(self.tokenizer, "apply_chat_template"):
            try:
                prompt = self.tokenizer.apply_chat_template(
                    self.conversation,
                    tokenize=False,
                    add_generation_prompt=True,
                )
                return prompt
            except Exception:
                pass

        # Fallback: ë‹¨ìˆœ í¬ë§·
        prompt = ""
        for msg in self.conversation:
            if msg["role"] == "user":
                prompt += f"User: {msg['content']}\n"
            else:
                prompt += f"Assistant: {msg['content']}\n"
        prompt += "Assistant: "
        return prompt

    # ----------------------------------------------------------------
    # ì±„íŒ…
    # ----------------------------------------------------------------
    def chat(self, user_input: str) -> str:
        """
        ì‚¬ìš©ì ì…ë ¥ â†’ ì‘ë‹µ ìƒì„±.

        Prefix caching í™œì„±í™”ë¡œ ì´ì „ ëŒ€í™”ì˜ KV cacheê°€ ì¬ì‚¬ìš©ë©ë‹ˆë‹¤.
        Stage ì „í™˜ ì§í›„ ì²« í„´ì—ëŠ” partial KV recomputationì´ ìë™ ì‹¤í–‰ë©ë‹ˆë‹¤.
        """
        self.conversation.append({"role": "user", "content": user_input})

        prompt = self._build_prompt()

        # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ê²½ê³ 
        token_ids = self.tokenizer.encode(prompt)
        if len(token_ids) > 1800:  # max_model_len=2048, ì—¬ìœ  í™•ë³´
            print(f"  [Warning] Conversation length ({len(token_ids)} tokens) "
                  f"approaching limit. Consider /reset.")

        outputs = self.llm.generate([prompt], self.sampling_params)
        response = outputs[0].outputs[0].text.strip()

        # KV snapshotì„ ìœ„í•´ vLLMì´ ì‹¤ì œ ì²˜ë¦¬í•œ ì •í™•í•œ token IDs ì €ì¥
        # (prompt_token_ids + generated_token_ids)
        # _build_prompt()ë¡œ ì¬í† í¬ë‚˜ì´ì§•í•˜ë©´ chat template ì°¨ì´ë¡œ í•´ì‹œ ë¶ˆì¼ì¹˜ ë°œìƒ
        self._last_generate_token_ids = (
            list(outputs[0].prompt_token_ids) +
            list(outputs[0].outputs[0].token_ids)
        )

        self.conversation.append({"role": "assistant", "content": response})
        return response

    # ----------------------------------------------------------------
    # Stage ì „í™˜ (Partial KV Recomputation)
    # ----------------------------------------------------------------
    def advance_to_stage2(self) -> bool:
        """
        Stage 1 â†’ Stage 2 ì „í™˜ (prefetch â†’ instant transition)

        Partial KV Recomputation:
        - Stage ì „í™˜ ì¦‰ì‹œ boundary layer ì„¤ì •
        - í˜„ì¬ ëŒ€í™”ë¥¼ ì¦‰ì‹œ ì¬ê³„ì‚°í•˜ì—¬ partial recompute ì‹¤í–‰
        - Boundary ì´ì „: KV-only (ë¹ ë¦„, ìºì‹œëœ hidden states ì‚¬ìš©)
        - Boundary ì´í›„: full forward (ì •í™•, ìƒˆ ê°€ì¤‘ì¹˜ ë°˜ì˜)
        """
        if self.current_stage >= 2:
            print("  Already at Stage 2 or higher.")
            return False

        stage_b_path = self.config.get("stage_b_checkpoint")
        if not stage_b_path or not os.path.exists(stage_b_path):
            print(f"  Stage B checkpoint not found: {stage_b_path}")
            return False

        # ğŸ”¥ Stage ì „í™˜ ì „: GPU persistent buffer â†’ CPU cache ë™ê¸°í™”
        self._sync_cache_before_transition()

        print("  [Stage 1 -> 2] Prefetching...")
        t0 = time.time()
        self.model.prefetch_stage2(stage_b_path)

        ready = self.model.wait_for_prefetch(timeout_s=120.0)
        if not ready:
            print("  Stage 2 prefetch failed or timed out.")
            return False

        # Instant transition (partial recompute boundary ìë™ ì„¤ì •ë¨)
        transitioned = self.model.advance_to_stage2_instant(wait_if_needed=False)
        if not transitioned:
            print("  Stage 2 instant transition failed.")
            return False

        self.current_stage = 2
        elapsed = time.time() - t0

        stage_info = self.model.get_stage_info()
        print(f"  âœ… Stage 2 transition complete ({elapsed:.2f}s)")
        print(f"  Active layers: {len(stage_info['active_layers'])}, "
              f"Progress: {stage_info['activation_progress']}")
        
        # =========================================================
        # ğŸ”¥ ì¶”ê°€í•  ë¶€ë¶„: vLLMì´ í”„ë¡¬í”„íŠ¸ë¥¼ ìë¥´ì§€ ëª»í•˜ê²Œ ìºì‹œ ê°•ì œ ì´ˆê¸°í™”
        # =========================================================
        if hasattr(self.llm, "reset_prefix_cache"):
            self.llm.reset_prefix_cache()

        # ğŸ”¥ CRITICAL: Trigger partial recompute NOW with current conversation
        # This ensures cached hidden states match the current prompt length
        self._trigger_partial_recompute()

        return True

    def advance_to_stage3(self) -> bool:
        """
        Stage 2 â†’ Stage 3 ì „í™˜ (prefetch â†’ instant transition)

        Partial KV Recomputation:
        - Stage ì „í™˜ ì¦‰ì‹œ boundary layer ì„¤ì •
        - í˜„ì¬ ëŒ€í™”ë¥¼ ì¦‰ì‹œ ì¬ê³„ì‚°í•˜ì—¬ partial recompute ì‹¤í–‰
        """
        if self.current_stage < 2:
            print("  Must be at Stage 2 first. Use /stage2.")
            return False
        if self.current_stage >= 3:
            print("  Already at Stage 3.")
            return False

        stage_c_path = self.config.get("stage_c_checkpoint")
        if not stage_c_path or not os.path.exists(stage_c_path):
            print(f"  Stage C checkpoint not found: {stage_c_path}")
            return False

        # ğŸ”¥ Stage ì „í™˜ ì „: GPU persistent buffer â†’ CPU cache ë™ê¸°í™”
        self._sync_cache_before_transition()

        print("  [Stage 2 -> 3] Prefetching...")
        t0 = time.time()
        self.model.prefetch_stage3(stage_c_path)

        ready = self.model.wait_for_prefetch(timeout_s=120.0)
        if not ready:
            print("  Stage 3 prefetch failed or timed out.")
            return False

        # Instant transition (partial recompute boundary ìë™ ì„¤ì •ë¨)
        transitioned = self.model.advance_to_stage3_instant(wait_if_needed=False)
        if not transitioned:
            print("  Stage 3 instant transition failed.")
            return False

        self.current_stage = 3
        elapsed = time.time() - t0

        stage_info = self.model.get_stage_info()
        print(f"  âœ… Stage 3 transition complete ({elapsed:.2f}s)")
        print(f"  Active layers: {len(stage_info['active_layers'])}, "
              f"Progress: {stage_info['activation_progress']}")

        # =========================================================
        # ğŸ”¥ ì¶”ê°€í•  ë¶€ë¶„: vLLMì´ í”„ë¡¬í”„íŠ¸ë¥¼ ìë¥´ì§€ ëª»í•˜ê²Œ ìºì‹œ ê°•ì œ ì´ˆê¸°í™”
        # =========================================================
        if hasattr(self.llm, "reset_prefix_cache"):
            self.llm.reset_prefix_cache()

        # ğŸ”¥ CRITICAL: Trigger partial recompute NOW with current conversation
        self._trigger_partial_recompute()

        return True

    # ----------------------------------------------------------------
    # Persistent Buffer â†’ CPU Cache ë™ê¸°í™”
    # ----------------------------------------------------------------
    def _sync_cache_before_transition(self):
        """
        Stage ì „í™˜ ì§ì „: GPU persistent bufferì— ëˆ„ì ëœ hidden statesë¥¼ CPUë¡œ ë™ê¸°í™”.

        - Persistent bufferì—ëŠ” prefill + decodeì˜ hidden statesê°€ index_copy_()ë¡œ ëˆ„ì 
        - í˜„ì¬ ëŒ€í™”ì˜ ì „ì²´ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ í•´ë‹¹ ë²”ìœ„ë§Œ CPUë¡œ ë³µì‚¬
        - ì´í›„ partial recomputeì—ì„œ CPU cacheë¥¼ ì‚¬ìš©
        """
        if not hasattr(self.model, 'model'):
            return

        inner_model = self.model.model
        if not hasattr(inner_model, 'sync_persistent_cache'):
            return

        # í˜„ì¬ ëŒ€í™”ì˜ í† í° ìˆ˜ ê³„ì‚°
        prompt = self._build_prompt()
        token_ids = self.tokenizer.encode(prompt)
        seq_len = len(token_ids)

        print(f"  [Sync] GPU buffer â†’ CPU cache ({seq_len} tokens)")
        inner_model.sync_persistent_cache(seq_len)

    # ----------------------------------------------------------------
    # KV Snapshot (Stage ì „í™˜ ì „ GPU ìºì‹œ ì§ì ‘ ì½ê¸°)
    # ----------------------------------------------------------------
    def _save_kv_snapshot(self, boundary_layer_idx: int):
        """
        Stage ì „í™˜ ì§ì „, GPU KV ìºì‹œ ë¸”ë¡ì—ì„œ layers 0~boundary-1ì˜ K,Vë¥¼
        ì§ì ‘ ì½ì–´ CPUì— ì €ì¥.

        ì›ë¦¬:
        - Stage Nê³¼ Stage N+1ì—ì„œ layers 0~boundary-1ì˜ weightsëŠ” ë™ì¼
        - ë”°ë¼ì„œ í•´ë‹¹ ë ˆì´ì–´ë“¤ì˜ K,V ê°’ë„ ë™ì¼
        - ì „í™˜ ì „ì— GPU blockì—ì„œ K,Vë¥¼ ì½ìœ¼ë©´ ì¬ê³„ì‚° ì—†ì´ ì¬ì‚¬ìš© ê°€ëŠ¥
        - Full blocks (block_size=16 ë‹¨ìœ„)ë§Œ ì €ì¥ (hashë¡œ block_id ì¡°íšŒ ê°€ëŠ¥)
        - Partial last blockì€ QKV_write_onlyë¡œ ì²˜ë¦¬ (K,V ê³„ì‚°í•˜ì§€ë§Œ attention ì—†ìŒ)

        ë°˜í™˜:
            (snapshot, num_full_tokens) ë˜ëŠ” (None, 0) on failure
        """
        try:
            from vllm.core.block.prefix_caching_block import PrefixCachingBlock
            import torch

            # 1. í† í° ID ê²°ì •
            #    í•µì‹¬: _build_prompt()ë¡œ ì¬í† í¬ë‚˜ì´ì§•í•˜ë©´ chat template í¬ë§· ì°¨ì´ë¡œ
            #    vLLMì´ ìºì‹±í•œ ì‹¤ì œ token IDsì™€ í•´ì‹œ ë¶ˆì¼ì¹˜ ë°œìƒ.
            #    â†’ chat()ì—ì„œ ì €ì¥í•œ ì‹¤ì œ token IDs ì‚¬ìš© (prompt_ids + generated_ids)
            if hasattr(self, '_last_generate_token_ids') and self._last_generate_token_ids:
                token_ids = self._last_generate_token_ids
                print(f"  [KVSnapshot] Using actual generate token IDs "
                      f"({len(token_ids)} tokens, exact match with vLLM cache)")
            else:
                prompt = self._build_prompt()
                token_ids = self.tokenizer.encode(prompt)
                print(f"  [KVSnapshot] âš ï¸  No saved token IDs, "
                      f"falling back to _build_prompt() ({len(token_ids)} tokens)")

            total_tokens = len(token_ids)

            # 2. Block ì„¤ì •
            block_size = self.llm.llm_engine.cache_config.block_size
            num_full_blocks = total_tokens // block_size
            num_full_tokens = num_full_blocks * block_size

            if num_full_blocks == 0:
                print(f"  [KVSnapshot] âš ï¸  No full blocks "
                      f"(total_tokens={total_tokens} < block_size={block_size})")
                return None, 0

            # 3. Block allocatorì—ì„œ cached_blocks ê°€ì ¸ì˜¤ê¸°
            #    ê²½ë¡œ: LLMEngine â†’ Scheduler â†’ SelfAttnBlockSpaceManager
            #          â†’ block_allocator (CpuGpuBlockAllocator)
            #          â†’ _allocators[Device.GPU] (PrefixCachingBlockAllocator)
            from vllm.utils import Device
            scheduler = self.llm.llm_engine.scheduler[0]
            gpu_alloc = scheduler.block_manager.block_allocator._allocators[Device.GPU]
            cached_blocks = gpu_alloc._cached_blocks  # Dict[hash, block_id]

            # 4. í† í° â†’ ë¸”ë¡ í•´ì‹œ ê³„ì‚° â†’ block_id ì¡°íšŒ (ìˆœì„œëŒ€ë¡œ)
            #    ì¤‘ê°„ì— blockì´ ì—†ìœ¼ë©´ abortí•˜ì§€ ì•Šê³  ê·¸ ì‹œì ê¹Œì§€ë§Œ ì‚¬ìš©
            block_ids = []
            prev_hash = None
            for i in range(num_full_blocks):
                chunk = token_ids[i * block_size: (i + 1) * block_size]
                bh = PrefixCachingBlock.hash_block_tokens(
                    is_first_block=(i == 0),
                    prev_block_hash=prev_hash,
                    cur_block_token_ids=chunk,
                    extra_hash=None,
                )
                bid = cached_blocks.get(bh)
                if bid is None:
                    print(f"  [KVSnapshot] âš ï¸  Block {i} (tokens {i*block_size}~"
                          f"{(i+1)*block_size-1}) not found â†’ "
                          f"using {i} blocks ({i*block_size} tokens)")
                    # abortí•˜ì§€ ì•Šê³  ì°¾ì€ ë¸”ë¡ê¹Œì§€ë§Œ ì‚¬ìš©
                    num_full_blocks = i
                    num_full_tokens = i * block_size
                    break
                block_ids.append(bid)
                prev_hash = bh

            if num_full_blocks == 0:
                print(f"  [KVSnapshot] âš ï¸  No blocks found â†’ fallback to QKV_write_only")
                return None, 0

            # 5. GPU KV ìºì‹œì—ì„œ K,V ì½ê¸° (layers 0~boundary-1)
            #    ê° ë ˆì´ì–´ì˜ Attention ê°ì²´: layer_wrapper.layer.self_attn.attn
            #    kv_cache[ve][0]: key cache [num_blocks, block_size, num_kv_heads, head_size]
            #    kv_cache[ve][1]: val cache [num_blocks, block_size, num_kv_heads, head_size]
            inner_model = self.model.model  # ProgressiveModelDualPath
            snapshot = {}

            for layer_idx in range(boundary_layer_idx):
                layer_wrapper = inner_model.layers[layer_idx]
                if not hasattr(layer_wrapper.layer, 'self_attn'):
                    continue
                attn_obj = layer_wrapper.layer.self_attn.attn  # Attention (vllm)
                kv = attn_obj.kv_cache[0]  # virtual engine 0
                # kv shape: [2, num_blocks, block_size, num_kv_heads, head_size]

                dev = kv.device
                bids_t = torch.tensor(block_ids, dtype=torch.long, device=dev)

                key_cache = kv[0]  # [num_blocks, block_size, num_kv_heads, head_size]
                val_cache = kv[1]

                # [num_full_blocks, block_size, num_kv_heads, head_size]
                k_blocks = key_cache[bids_t]
                v_blocks = val_cache[bids_t]

                # [num_full_tokens, num_kv_heads, head_size] â†’ CPU
                k_all = k_blocks.reshape(num_full_tokens, *key_cache.shape[2:]).cpu()
                v_all = v_blocks.reshape(num_full_tokens, *val_cache.shape[2:]).cpu()

                snapshot[layer_idx] = (k_all, v_all)

            print(f"  [KVSnapshot] âœ… Snapshot saved: {len(snapshot)} layers Ã— "
                  f"{num_full_blocks} blocks Ã— {block_size} = {num_full_tokens} tokens  "
                  f"[GPUâ†’CPU memcopy, 0 FLOPs]")
            return snapshot, num_full_tokens

        except Exception as e:
            print(f"  [KVSnapshot] âš ï¸  Failed to save snapshot: {e}")
            import traceback
            traceback.print_exc()
            return None, 0

    # ----------------------------------------------------------------
    # Partial Recompute íŠ¸ë¦¬ê±°
    # ----------------------------------------------------------------
    def _clear_kv_prefix_cache(self) -> None:
        """
        Stage ì „í™˜ í›„ stale KV prefix cache blocks í‡´ì¶œ.

        Stage ì „í™˜ ì‹œ weightsê°€ ë³€ê²½ë˜ë¯€ë¡œ ê¸°ì¡´ì— ìºì‹±ëœ KV blocksëŠ”
        ì˜ëª»ëœ ê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. _trigger_partial_recompute() ì „ì—
        ë°˜ë“œì‹œ í˜¸ì¶œí•˜ì—¬ stale blocksë¥¼ í‡´ì¶œí•œ í›„ ì˜¬ë°”ë¥¸ K,Vë¡œ ë‹¤ì‹œ ì±„ì›ë‹ˆë‹¤.

        vLLM LLM.reset_prefix_cache() â†’ LLMEngine â†’ Scheduler â†’ BlockManager ìˆœìœ¼ë¡œ
        ë‚´ë¶€ì ìœ¼ë¡œ PrefixCachingBlockAllocator._cached_blocks.clear()ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        """
        try:
            success = self.llm.reset_prefix_cache()
            if success:
                print(f"  [KVCache] âœ… Prefix cache evicted (stale blocks removed)")
            else:
                print(f"  [KVCache] âš ï¸ reset_prefix_cache() returned False "
                      f"(prefix caching may not be active or blocks still in use)")
        except Exception as e:
            print(f"  [KVCache] âš ï¸ Could not clear prefix cache: {e}")

    def _trigger_partial_recompute(self):
        """
        Stage ì „í™˜ ì§í›„ í˜„ì¬ ëŒ€í™”ë¥¼ ì¬ê³„ì‚°í•˜ì—¬ partial KV recompute ì‹¤í–‰.

        í•µì‹¬ ì›ë¦¬ (KV Snapshot ìµœì í™”):
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Front layers (0~boundary-1): weights ë¶ˆë³€ â†’ K,V ë™ì¼           â”‚
        â”‚   STEP A: GPU KV ìºì‹œì—ì„œ K,Vë¥¼ ì§ì ‘ ì½ì–´ CPUì— ì €ì¥ (snapshot) â”‚
        â”‚   STEP B: reset_prefix_cache() â†’ stale blocks í‡´ì¶œ             â”‚
        â”‚   STEP C: generate() â†’ partial recompute:                      â”‚
        â”‚     - Full blocks: snapshot memcopy â†’ KV cache (0 FLOPs)       â”‚
        â”‚     - Partial block: QKV proj + rope â†’ KV cache (flash_attn ìƒëµ)â”‚
        â”‚   ê²°ê³¼: front layers K,Vê°€ ì¬ê³„ì‚° ì—†ì´ ìƒˆ blocksì— ë³µì›ë¨       â”‚
        â”‚                                                                 â”‚
        â”‚ Back layers (boundary~end): weights ë³€ê²½ë¨                      â”‚
        â”‚   STEP Cì—ì„œ full forward â†’ ìƒˆ K,V ê³„ì‚° ë° ì €ì¥                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        ë™ì‘ ìˆœì„œ:
        1. _sync_cache_before_transition(): GPU hidden states â†’ CPU cache
        2. Stage ì „í™˜ â†’ boundary ì„¤ì • (advance_to_stage*_instant)
        3. ğŸ”¥ _save_kv_snapshot(): GPU KV ë¸”ë¡ ì§ì ‘ ì½ê¸° â†’ CPU ì €ì¥
        4. ğŸ”¥ _clear_kv_prefix_cache(): stale blocks í‡´ì¶œ (reset_prefix_cache)
        5. model.set_kv_snapshot(): snapshotì„ progressive modelì— ì „ë‹¬
        6. generate() â†’ forward() partial recompute ì‹¤í–‰:
           - Front: snapshot/QKV_write_only â†’ K,V ë³µì› (hidden states from CPU)
           - Back: full forward â†’ K,V ì¬ê³„ì‚° (hidden states computed)
        7. ìƒˆ K,Vê°€ prefix cacheì— ì €ì¥ë¨ â†’ ë‹¤ìŒ generate()ì—ì„œ prefill ìŠ¤í‚µ
        """
        if len(self.conversation) == 0:
            print(f"  [PartialRecompute] No conversation history, skipping")
            return

        print(f"\n  [PartialRecompute] Triggering with current conversation...")
        print(f"  Conversation turns: {len(self.conversation) // 2}")

        # ğŸ”¥ Step 1: boundary í™•ì¸ (set_partial_recompute()ì—ì„œ ì´ë¯¸ ì„¤ì •ë¨)
        inner_model = self.model.model  # ProgressiveModelDualPath
        boundary = inner_model._partial_recompute_boundary
        if boundary is None:
            print(f"  [PartialRecompute] No boundary set, skipping")
            return

        # ğŸ”¥ Step 2: GPU KV ìºì‹œì—ì„œ K,V snapshot ì €ì¥ (reset ì „ì— í•´ì•¼ í•¨!)
        print(f"  [Step 2] Saving KV snapshot from GPU cache (layers 0~{boundary-1})...")
        snapshot, num_full_tokens = self._save_kv_snapshot(boundary)

        # ğŸ”¥ Step 3: Stale KV prefix cache blocks í‡´ì¶œ
        print(f"  [Step 3] Evicting stale KV prefix cache blocks...")
        self._clear_kv_prefix_cache()

        # ğŸ”¥ Step 4: Snapshotì„ progressive modelì— ì „ë‹¬
        print(f"  [Step 4] Passing KV snapshot to progressive model...")
        inner_model.set_kv_snapshot(snapshot, num_full_tokens)

        # í˜„ì¬ ëŒ€í™” ê¸°ë¡ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_prompt()
        token_ids = self.tokenizer.encode(prompt)
        print(f"  Prompt tokens: {len(token_ids)} "
              f"(full-block tokens: {num_full_tokens}, "
              f"partial: {len(token_ids) - num_full_tokens})")

        # ìµœì†Œ ìƒì„±ìœ¼ë¡œ partial recompute íŠ¸ë¦¬ê±°
        # max_tokens=1: forward pass + KV cache writeë§Œ í•„ìš”
        minimal_params = SamplingParams(temperature=0.0, max_tokens=1)

        print(f"  [Step 5] Running partial recompute generate()...")
        t0 = time.time()

        # generate() í˜¸ì¶œ:
        # - prefix cache miss (reset í›„) â†’ vLLMì´ prefill ì‹¤í–‰
        # - forward()ì—ì„œ partial recompute ëª¨ë“œ ë™ì‘ (per-layer ë¡œê·¸ ì¶œë ¥ë¨):
        #   * Front layers: KV snapshot memcopy + QKV_write_only fallback
        #   * Back layers: full forward (ìƒˆ ê°€ì¤‘ì¹˜)
        # - ì™„ë£Œ í›„ K,Vê°€ prefix cacheì— ì €ì¥ë¨ â†’ ë‹¤ìŒ generate()ì—ì„œ prefill ìŠ¤í‚µ
        self.llm.generate([prompt], minimal_params)

        elapsed = time.time() - t0
        print(f"  âœ… Partial recomputation complete ({elapsed:.2f}s)")
        print(f"  ğŸ“Œ Front layers: K,V REUSED from snapshot (0 FLOPs for full blocks)")
        print(f"  ğŸ“Œ Back  layers: K,V RECOMPUTED with new weights")
        print(f"  ğŸ“Œ Prefix cache populated â†’ next generate() will skip prefill\n")

    # ----------------------------------------------------------------
    # ìƒíƒœ / ë¦¬ì…‹
    # ----------------------------------------------------------------
    def reset_conversation(self):
        """
        ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”.

        Hidden state cacheë„ í•¨ê»˜ í´ë¦¬ì–´ë©ë‹ˆë‹¤.
        """
        self.conversation = []

        # Hidden state cache + persistent buffer í´ë¦¬ì–´
        if hasattr(self.model, 'model'):
            inner = self.model.model
            if hasattr(inner, 'clear_hidden_cache'):
                inner.clear_hidden_cache()
            if hasattr(inner, 'clear_persistent_buffers'):
                inner.clear_persistent_buffers()
            print("  Conversation, hidden cache, and persistent buffers reset.")
        else:
            print("  Conversation reset.")

    def print_status(self):
        """í˜„ì¬ ìƒíƒœ ì¶œë ¥"""
        stage_info = self.model.get_stage_info()

        # Partial recompute ìƒíƒœ í™•ì¸
        partial_mode = False
        if hasattr(self.model, 'model'):
            inner_model = self.model.model
            if hasattr(inner_model, '_partial_recompute_boundary'):
                boundary = inner_model._partial_recompute_boundary
                partial_mode = boundary is not None

        print(f"\n  {'='*50}")
        print(f"  Model:    {self.model_name}")
        print(f"  Stage:    {self.current_stage}")
        print(f"  Active:   {len(stage_info['active_layers'])} layers")
        print(f"  Inactive: {len(stage_info['inactive_layers'])} layers")
        print(f"  Progress: {stage_info['activation_progress']}")
        print(f"  Turns:    {len(self.conversation) // 2}")
        print(f"  GPU Mem:  {torch.cuda.memory_allocated() / (1024**3):.2f} GB")
        print(f"  Partial Recompute: {'Active' if partial_mode else 'Idle'}")
        print(f"  {'='*50}")


# ============================================================================
# Main
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Progressive Serving Chatbot (Partial KV Cache Recomputation)"
    )
    parser.add_argument(
        "--model",
        type=str,
        choices=list(MODELS.keys()),
        default="llama",
        help="Model to use (default: llama)",
    )
    args = parser.parse_args()

    print("\n" + "=" * 60)
    print("Progressive Serving Chatbot - Partial KV Recomputation")
    print(f"  Model: {args.model}")
    print(f"  GPU:   {torch.cuda.get_device_name(0)}")
    print("=" * 60)

    chatbot = ProgressiveChatbotPartial(args.model)

    print(f"\n{'='*60}")
    print(f"  Ready! (Stage {chatbot.current_stage})")
    print(f"  Commands: /stage2, /stage3, /status, /reset, /quit")
    print(f"  ğŸš€ KV Cache persists between turns (prefix caching)")
    print(f"  ğŸš€ Partial recomputation on stage transitions")
    print(f"{'='*60}\n")

    while True:
        try:
            user_input = input(f"You [Stage {chatbot.current_stage}]: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nBye!")
            break

        if not user_input:
            continue

        # ëª…ë ¹ì–´ ì²˜ë¦¬
        if user_input == "/quit":
            print("Bye!")
            break
        elif user_input == "/stage2":
            chatbot.advance_to_stage2()
            continue
        elif user_input == "/stage3":
            chatbot.advance_to_stage3()
            continue
        elif user_input == "/status":
            chatbot.print_status()
            continue
        elif user_input == "/reset":
            chatbot.reset_conversation()
            continue

        # ì±„íŒ…
        t0 = time.time()
        response = chatbot.chat(user_input)
        elapsed = time.time() - t0

        print(f"Assistant [Stage {chatbot.current_stage}] ({elapsed:.1f}s): {response}\n")


if __name__ == "__main__":
    main()
